---
title: "Practical machine learning course project"
output: 
  html_document:
        keep_md: true
---
## Background of this project
The purpose of this project is to predict how well the participant has lifted the dumbell (grade A,B,C,D,E) based on measurements from accelerometers on the belt, forearm, arm etc.

The data is from this source: http://groupware.les.inf.puc-rio.br/har.

Training data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
Test data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


## Import all the packages and the data sets
```{r,warning=FALSE,message=FALSE}
library(caret);library(randomForest)
```

```{r}
if(!exists("training_data")){
  training_data <- read.csv(file.path(getwd(),"pml-training.csv")
                                      ,header=TRUE,stringsAsFactors = FALSE)
                            
}

if(!exists("testing_data")){
  testing_data <- read.csv(file.path(getwd(),"pml-testing.csv")
                                     ,header=TRUE,stringsAsFactors = FALSE)
}

```


## Eliminate the NA & blank columns

From an inspection of the data, we can see that there are many predictors with NA or no values (blank). We will not attempt to impute any values and will not consider these variables when building our model.

Furthermore, the first 7 predictors (columns) of the data do not seem to have any impact on the classe outcome, for example, username, timestamp etc. We shall also not consider these data when building our model.

```{r}
countBlanksAndNA <- function(cnames,training_data){
      na_count <- numeric()
      blank_count <- numeric()
      for(i in 1:length(cnames)){
        na_count<- c(na_count,sum(is.na(training_data[[cnames[i]]])))
        blank_count <- c(blank_count,sum(training_data[[cnames[i]]]==""))
      }
      return(data.frame(column_names=cnames,countOfNa=na_count,blankCount=blank_count))
}

blankOrNACount <- countBlanksAndNA(colnames(training_data),training_data)
columnsWoBlanksOrNA <- as.character(blankOrNACount[blankOrNACount$countOfNa==0&blankOrNACount$blankCount==0,1])

#Remove the first 7 columns which have no relation to the outcome eg. username, timestamp etc
columnsWoBlanksOrNA <- columnsWoBlanksOrNA[-c(1:7)]
training_dataCleaned <- training_data[,columnsWoBlanksOrNA]
training_dataCleaned$classe <- as.factor(training_dataCleaned$classe)
testing_dataCleaned <- testing_data[,columnsWoBlanksOrNA[-53]]
```

## Check for predictors with zero variance

None of the predictors remaining have zero variance
```{r}
nsv <- nearZeroVar(training_dataCleaned,saveMetrics = TRUE)
print(nsv)
```

## Data pre-processing (standardising)

We will standardise the training data set and apply the same standardisation to the testing data set.

```{r}
preObj <- preProcess(training_dataCleaned[,-53],method=c("center","scale"))
training_dataCleaned <- cbind(predict(preObj,training_dataCleaned[,-53]),classe=training_dataCleaned$classe)
testing_dataCleaned <- predict(preObj,testing_dataCleaned)
```

## Use k-fold cross validation to predict the out of sample error rate

We will segregate our training data into 10 different sets. Each set will have 10% of the data for validation and 90% of the data for training.

```{r}
set.seed(5)
folds <- createFolds(y=training_dataCleaned$classe,k=10,list=TRUE,returnTrain=TRUE)
str(folds)
```

## Prediction using Classification Tree

We will use k-fold cross validation to predict the out of sample error rate for the classification tree model. 

We have separated our training data into 10 fold ie. validation data set is 10% while the training data set is 90%. We have 10 different training-validation sets.

For each training-validation set, we will train our model on the training set and use our model to predict the classe outcome in the validation set. We will then compute the accuracy rate of our prediction.

The estimated out of sample accuracy rate is then the average of the 10 accuracy rates.

```{r,cache=TRUE}
pred_acc_tree <- data.frame()
for(f in folds){

    trainingSet <- training_dataCleaned[f,]
    testingSet <- training_dataCleaned[-f,]
    
    model_tree <- train(classe~.,method="rpart",data=trainingSet)
    prediction_tree <- predict(model_tree,newdata=testingSet)
    
    cm <- confusionMatrix(prediction_tree,testingSet$classe)
    pred_acc_tree <- rbind(pred_acc_tree,cm$overall["Accuracy"])
    
}

colnames(pred_acc_tree) <- "Prediction Accuracy"
print(pred_acc_tree)
```

### Expected out of sample error rate (Classification tree)
The expected out of sample error rate is 1-accuracy rate

```{r}
# Average accuracy rate of the model based on 10 training-validation sets
mean(pred_acc_tree[,1])

# Average out of sample error rate
1-mean(pred_acc_tree[,1])
```


## Prediction using Random Forest
We will use k-fold cross validation to predict the out of sample error rate for the random forest model. 

We have separated our training data into 10 fold ie. validation data set is 10% while the training data set is 90%. We have 10 different training-validation sets.

For each training-validation set, we will train our model on the training set and use our model to predict the classe outcome in the validation set. We will then compute the accuracy rate of our prediction.

The estimated out of sample accuracy rate is then the average of the 10 accuracy rates.

```{r,cache=TRUE}
pred_acc_RF <- data.frame()
for(f in folds){
  
  trainingSet <- training_dataCleaned[f,]
  testingSet <- training_dataCleaned[-f,]
  
  model_RF <- randomForest(classe~.,data=trainingSet)
  prediction_RF <- predict(model_RF,newdata=testingSet)
  
  cm <- confusionMatrix(prediction_RF,testingSet$classe)
  pred_acc_RF <- rbind(pred_acc_RF,cm$overall["Accuracy"])
  
}
colnames(pred_acc_RF)<- "Prediction Accuracy"
print(pred_acc_RF)
```

### Expected out of sample error rate (Random Forest Model)
The expected out of sample error rate is 1-accuracy rate

```{r}
# Average accuracy rate of the RF model based on 10 training-validation sets
mean(pred_acc_RF[,1])

# Average out of sample error rate
1-mean(pred_acc_RF[,1])

```


## Choice and application of the model to our out-of-sample test data
Since the accuracy rate of the random forest model is much better than the classification tree, we will choose the random forest model to predict the outcomes in the test data set.

We will train the model using the entire training data set and use the model for prediction.


```{r}
final_model <- randomForest(classe~.,data=training_dataCleaned)
final_prediction <- predict(final_model,newdata=testing_dataCleaned)

# Our final prediction of the classe variable is:
print(final_prediction)
```




